{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDNt599/jPQyVAHeqLBxb1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"I03YitINgC9W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715358357214,"user_tz":-60,"elapsed":8953,"user":{"displayName":"by tan","userId":"09012215015310927543"}},"outputId":"a26c11ea-20fb-47c8-96d1-8f6b81b53951"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import h5py\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import Word2Vec\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.translate.bleu_score import corpus_bleu\n","import time\n","# Download NLTK Resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Preloaded stop words\n","stop_words = set(stopwords.words('english'))\n","\n","# Pre-compiled regular expressions\n","html_tags_regex = re.compile(r'<.*?>')\n","non_alpha_numeric_regex = re.compile(r'[^a-zA-Z\\s]')\n","\n","#Text Preprocessing\n","def preprocess_text(text):\n","    text = html_tags_regex.sub('', text)\n","    text = non_alpha_numeric_regex.sub('', text)\n","    text = text.lower()\n","    words = word_tokenize(text)\n","    words = [word for word in words if word not in stop_words]\n","    lemmatizer = WordNetLemmatizer()\n","    return [lemmatizer.lemmatize(word) for word in words]\n","\n","# Load data\n","data = pd.read_csv('chat_health.csv')\n","data =data.head(2000)\n","data['original_answer'] = data['short_answer']\n","data['tokenized_questions'] = data['short_question'].apply(preprocess_text)\n","\n"]},{"cell_type":"code","source":["# Training the Word2Vec model\n","model = Word2Vec(sentences=data['tokenized_questions'], vector_size=100, window=5, min_count=1, workers=4)\n","model.save(\"word2vec_model_2000_pairs.model\")\n","model = Word2Vec.load(\"word2vec_model_2000_pairs.model\")\n","\n","# Get a vector representation of the problem from the word list\n","def get_question_vector(words, model):\n","    return np.mean([model.wv[word] for word in words if word in model.wv], axis=0)\n","\n","# Get the word vector of the text\n","data['question_vec'] = data['tokenized_questions'].apply(lambda words: get_question_vector(words, model))\n","\n","# Save vector data to HDF5 file\n","with h5py.File('question_vectors_2000_pairs.h5', 'w') as f:\n","    vectors = np.stack(data['question_vec'].dropna(), axis=0)\n","    f.create_dataset('vectors', data=vectors)\n","    dt = h5py.string_dtype(encoding='utf-8')\n","    f.create_dataset('answers', data=data['original_answer'].to_numpy(dtype=dt))"],"metadata":{"id":"YaaavseM0IEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import h5py\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","\n","best_index = None\n","\n","def generate_answer(question, model):\n","    question_words = preprocess_text(question)\n","    question_vec = get_question_vector(question_words, model)\n","    with h5py.File('question_vectors_2000_pairs.h5', 'r') as f:\n","        vectors = f['vectors'][:]\n","        cosine_similarities = cosine_similarity([question_vec], vectors)\n","        best_index = np.argmax(cosine_similarities)\n","        best_answer = f['answers'][best_index].decode('utf-8')\n","    return best_answer\n","\n"],"metadata":{"id":"iFt_Zwm9xVAU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testData = pd.read_csv('testset.csv')"],"metadata":{"id":"YflWrE3brWLk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testData['generated_answer'] = testData['short_question'].apply(lambda q: generate_answer(q, model))\n","\n","references = testData['original_answer'].apply(lambda a: [a.split()]).tolist()\n","candidates = testData['generated_answer'].apply(lambda a: a.split()).tolist()\n","\n","bleu_score = corpus_bleu(references, candidates)\n","print(\"BLEU Score:\", bleu_score)"],"metadata":{"id":"cSDZXm0no1aJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715183206463,"user_tz":-60,"elapsed":781,"user":{"displayName":"by tan","userId":"09012215015310927543"}},"outputId":"ccf7b42d-3d6a-49ce-b461-e3c70caa9967"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BLEU Score: 0.761963965471042\n"]}]},{"cell_type":"code","source":["start_time = time.time()\n","testData['generated_answer'] = testData['short_question'].head(50).apply(lambda q: generate_answer(q, model))\n","end_time = time.time()\n","response_time = end_time - start_time\n","average_response_time = response_time / 50\n","\n","print(f\"Average response time per record: {average_response_time} seconds\")"],"metadata":{"id":"iu5kOcBeqPsi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715183208604,"user_tz":-60,"elapsed":324,"user":{"displayName":"by tan","userId":"09012215015310927543"}},"outputId":"7c3cd6bb-ac76-4f5b-a6b6-231d7ed900ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average response time per record: 0.005222439765930176 seconds\n"]}]}]}